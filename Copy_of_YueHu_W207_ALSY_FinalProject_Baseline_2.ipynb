{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of YueHu_W207_ALSY_FinalProject_Baseline_2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shirleyhywl/hello-world/blob/master/Copy_of_YueHu_W207_ALSY_FinalProject_Baseline_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9V1hXKgnTnQ",
        "colab_type": "text"
      },
      "source": [
        "# Data and Plan\n",
        "\n",
        "## Andrew Webb, Laura Chutny, Suzy Choi, Yue Hu\n",
        "### 1. From Kaggle\n",
        "Each predicted keypoint is specified by an (x,y) real-valued pair in the space of pixel indices. There are 15 keypoints, which represent the following elements of the face:\n",
        "\n",
        "left_eye_center  \n",
        "right_eye_center  \n",
        "left_eye_inner_corner  \n",
        "left_eye_outer_corner  \n",
        "right_eye_inner_corner  \n",
        "right_eye_outer_corner  \n",
        "left_eyebrow_inner_end  \n",
        "left_eyebrow_outer_end  \n",
        "right_eyebrow_inner_end  \n",
        "right_eyebrow_outer_end  \n",
        "nose_tip  \n",
        "mouth_left_corner  \n",
        "mouth_right_corner  \n",
        "mouth_center_top_lip  \n",
        "mouth_center_bottom_lip  \n",
        "\n",
        "Left and right here refers to the point of view of the subject.  \n",
        "\n",
        "In some examples, some of the target keypoint positions are missing (encoded as missing entries in the csv, i.e., with nothing between two commas).\n",
        "\n",
        "The input image is given in the last field of the data files, and consists of a list of pixels (ordered by row), as integers in (0,255). The images are 96x96 pixels.\n",
        "\n",
        "#### Data files - have been uploaded to GitHub Repo\n",
        "- training.csv: list of training 7049 images. Each row contains the (x,y) coordinates for 15 keypoints, and image data as row-ordered list of pixels.  \n",
        "- test.csv: list of 1783 test images. Each row contains ImageId and image data as row-ordered list of pixels\n",
        "- submissionFileFormat.csv: list of 27124 keypoints to predict. Each row contains a RowId, ImageId, FeatureName, Location. FeatureName are \"left_eye_center_x,\" \"right_eyebrow_outer_end_y,\" etc. Location is what you need to predict."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nqq3gpbnYox",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### 2. Plan\n",
        "1. Data Cleaning\n",
        "- TBD based on the tutorial\n",
        "2. Split data - use training set and split into train, dev and test.\n",
        "\n",
        "3. kNN; MN Naive Bayes; Logistic Regression --> run and compare as base\n",
        "\n",
        "4. Maybe run\n",
        "\n",
        "5. Then learn and run Neural Net to classify"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PXOYpA7ncLW",
        "colab_type": "text"
      },
      "source": [
        "### 3. Baseline\n",
        "Due Week 10: (July 8)\n",
        "- Plan for project\n",
        "- Do Tutorial\n",
        "- EDA / Cleaning\n",
        "- More detailed plan around project (maybe a pseudocode type plan)\n",
        "- check out Google colab - using this\n",
        "- Theano - install - no need when using colab\n",
        "\n",
        "Week 11/12 (July 15 and 22)\n",
        "\n",
        "Week 12 - Checkin with Instructor  (July 22)\n",
        "\n",
        "Week 14 - Project due (August 5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiNRFDnbmh8J",
        "colab_type": "text"
      },
      "source": [
        "### 4. Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRC7KmyqkJfZ",
        "colab_type": "code",
        "outputId": "2583e36f-35c2-431f-fc8e-b78479d1d1f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# ADD OTHER IMPORTS HERE AS NECESSARY\n",
        "# This tells matplotlib not to try opening a new window for each plot.\n",
        "%matplotlib inline\n",
        "\n",
        "# General libraries.\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import time\n",
        "import sys\n",
        "import csv\n",
        "import os\n",
        "\n",
        "# SK-learn libraries for learning.\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "# SK-learn libraries for evaluation.\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Theano imports\n",
        "import theano \n",
        "from theano import tensor as T\n",
        "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
        "print(theano.config.device) # We're using CPUs (for now)\n",
        "print(theano.config.floatX) # Should be 64 bit for CPUs\n",
        "np.random.seed(0) # Setting random seed\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cpu\n",
            "float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExLN-9AdQABB",
        "colab_type": "code",
        "outputId": "aba545ce-3b9d-4c1a-96c6-656c92ccc3ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        }
      },
      "source": [
        "print(\"python version =\",sys.version)\n",
        "print(\"Pandas Version = \",pd.__version__)\n",
        "print(\"Numpy Version = \",np.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "python version = 3.6.8 (default, Jan 14 2019, 11:02:34) \n",
            "[GCC 8.0.1 20180414 (experimental) [trunk revision 259383]]\n",
            "Pandas Version =  0.24.2\n",
            "Numpy Version =  1.16.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7w4tZMF-Mfy",
        "colab_type": "code",
        "outputId": "2fde2b6a-8410-4980-c47d-4f3979ea2d8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Run this cell to mount your Google Drive.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBWFpL1VSvJb",
        "colab_type": "code",
        "outputId": "7444c8f6-713e-4982-d0bd-a7af3ab64d1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "os.getcwd()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-J5l7OKPCYtt",
        "colab_type": "text"
      },
      "source": [
        "### 5. Data Import and Cleaning\n",
        " - There is a Train and Test sample in the data from Kaggle\n",
        " - We should split the Train data into Train and Dev - and optimize on Dev; Save Test for the very last step in each method.\n",
        " - Select an appropriate scoring method (per the Kaggle contest and maybe one other as well)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "598ap69mKryj",
        "colab_type": "text"
      },
      "source": [
        "###Train Data (training.csv)\n",
        "\n",
        "*  There is total 31 columns (features) and 7049 rows (images)\n",
        "  \n",
        "    *  30 columns for 15 predicted keypoint pair specified by a (x,y) real-value pair in the space of pixel indices\n",
        "    * 1 column for 96*96 pixel input image consists a list of pixels (ordered by row), as integers in (0,255) in string type\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGJ88PhRKifz",
        "colab_type": "code",
        "outputId": "6c81f564-3ca0-40ca-8b95-3f5edce28224",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 618
        }
      },
      "source": [
        "def Data_PreProcess(train = True):\n",
        "  \"\"\" Loads data based on the purpose of training or testing (train=True, test=FALSE).\n",
        "  Tranform last column of imagine information from string to numpy array.\n",
        "  Finally, normalized keypoint pairs based on each keypoint average.\"\"\"\n",
        "  \n",
        "  # load data \n",
        "  if train: \n",
        "    data = pd.read_csv('drive/My Drive/FinalProject_w207/training.csv')\n",
        "    df = pd.DataFrame(data)\n",
        "  else:\n",
        "    data = pd.read_csv('drive/My Drive/FinalProject_w207/test.csv')\n",
        "    df = pd.DataFrame(data)\n",
        "  print('There is %i columns (features)' % df.shape[0])\n",
        "  print('There is %i rows (images)' % df.shape[1])\n",
        "  print(df.count())\n",
        "  \n",
        "  # remove the rows which contain NA value\n",
        "  df = df.dropna()\n",
        "  print('There is %i rows (images) after remove NA values' % df.shape[0])\n",
        "  \n",
        "  # transform image data and load as X value\n",
        "  df['Image']=df['Image'].apply(lambda X: np.fromstring(X, sep=' ')) # change string in 'image' column to nparray\n",
        "  image= np.vstack(df['Image'].to_numpy())   # Convert 'image'column to nparray\n",
        "  X = image/255\n",
        "  \n",
        "  \n",
        "  # Normalize keypoint pairs and load as Y value\n",
        "  keypoints = df[list(df.columns)[:-1]].to_numpy()\n",
        "  mean = df[list(df.columns)[:-1]].mean(axis=1).to_numpy()\n",
        "  mean_array = np.repeat(mean.reshape(df.shape[0],1),30,axis =1)\n",
        "  Y= (keypoints-mean_array)/mean_array\n",
        "  \n",
        "  return X,Y\n",
        "\n",
        "X,Y = Data_PreProcess()\n",
        "print(\"X.shape == {}; X.min == {:.3f}; X.max == {:.3f}\".format(\n",
        "    X.shape, X.min(), X.max()))\n",
        "print(\"Y.shape == {}; Y.min == {:.3f}; Y.max == {:.3f}\".format(\n",
        "    Y.shape, Y.min(), Y.max()))\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There is 7049 columns (features)\n",
            "There is 31 rows (images)\n",
            "left_eye_center_x            7039\n",
            "left_eye_center_y            7039\n",
            "right_eye_center_x           7036\n",
            "right_eye_center_y           7036\n",
            "left_eye_inner_corner_x      2271\n",
            "left_eye_inner_corner_y      2271\n",
            "left_eye_outer_corner_x      2267\n",
            "left_eye_outer_corner_y      2267\n",
            "right_eye_inner_corner_x     2268\n",
            "right_eye_inner_corner_y     2268\n",
            "right_eye_outer_corner_x     2268\n",
            "right_eye_outer_corner_y     2268\n",
            "left_eyebrow_inner_end_x     2270\n",
            "left_eyebrow_inner_end_y     2270\n",
            "left_eyebrow_outer_end_x     2225\n",
            "left_eyebrow_outer_end_y     2225\n",
            "right_eyebrow_inner_end_x    2270\n",
            "right_eyebrow_inner_end_y    2270\n",
            "right_eyebrow_outer_end_x    2236\n",
            "right_eyebrow_outer_end_y    2236\n",
            "nose_tip_x                   7049\n",
            "nose_tip_y                   7049\n",
            "mouth_left_corner_x          2269\n",
            "mouth_left_corner_y          2269\n",
            "mouth_right_corner_x         2270\n",
            "mouth_right_corner_y         2270\n",
            "mouth_center_top_lip_x       2275\n",
            "mouth_center_top_lip_y       2275\n",
            "mouth_center_bottom_lip_x    7016\n",
            "mouth_center_bottom_lip_y    7016\n",
            "Image                        7049\n",
            "dtype: int64\n",
            "There is 2140 rows (images) after remove NA values\n",
            "X.shape == (2140, 9216); X.min == 0.000; X.max == 1.000\n",
            "Y.shape == (2140, 30); Y.min == -0.919; Y.max == 1.090\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0y7FSBr1bU3M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create Dev Data out of the Train data set."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0L9qaQoXjs9",
        "colab_type": "text"
      },
      "source": [
        "### Single Hidden Layer\n",
        "Create a neural net with a single hidden layer\n",
        "\n",
        "*   **Activation Function** ReLU (Rectified Linear Unit) y= max(0,X)\n",
        "https://medium.com/@danqing/a-practical-guide-to-relu-b83ca804f1f7\n",
        "*  **Cost Function** Mean Squared Error (MSE)\n",
        "*  **Optimization Method** Nesterov_momentum gradient descent batch optimization with learning rate of 0.01 and momentum parameters of 0.9. The maximum iteration is set to 400.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZ97RC7Mfsqg",
        "colab_type": "code",
        "outputId": "13ce44b5-7076-4438-9a4d-5f75bef7590d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        }
      },
      "source": [
        "pip install git+https://github.com/Lasagne/Lasagne"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/Lasagne/Lasagne\n",
            "  Cloning https://github.com/Lasagne/Lasagne to /tmp/pip-req-build-lbbyt_8p\n",
            "  Running command git clone -q https://github.com/Lasagne/Lasagne /tmp/pip-req-build-lbbyt_8p\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from Lasagne==0.2.dev1) (1.16.4)\n",
            "Building wheels for collected packages: Lasagne\n",
            "  Building wheel for Lasagne (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-59gv8zml/wheels/c4/20/90/9f7242c381402829c5918261e3eb51a87bc1d8521456749b57\n",
            "Successfully built Lasagne\n",
            "Installing collected packages: Lasagne\n",
            "Successfully installed Lasagne-0.2.dev1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rg7xDsudjqul",
        "colab_type": "code",
        "outputId": "228e9ec4-e7f1-4c90-8ce0-1185bc886005",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "source": [
        "pip install nolearn"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nolearn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/69/2882491c14c58431c06d5a12a007eefcc4fb3f5ac7af624a5d212b0bbdd4/nolearn-0.6.0.tar.gz (320kB)\n",
            "\u001b[K     |████████████████████████████████| 327kB 2.8MB/s \n",
            "\u001b[?25hCollecting gdbn (from nolearn)\n",
            "  Downloading https://files.pythonhosted.org/packages/88/5c/512341ac1e6e8fa3008171b8dae49d2bd7b886e79f525658955bc59946c1/gdbn-0.1.tar.gz\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from nolearn) (0.13.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from nolearn) (0.21.2)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from nolearn) (0.8.3)\n",
            "Requirement already satisfied: Lasagne in /usr/local/lib/python3.6/dist-packages (from nolearn) (0.2.dev1)\n",
            "Requirement already satisfied: Theano in /usr/local/lib/python3.6/dist-packages (from nolearn) (1.0.4)\n",
            "Collecting gnumpy (from gdbn->nolearn)\n",
            "  Downloading https://files.pythonhosted.org/packages/46/63/2c7f2fd6763130700dff21aada32b2d6cf3373a625af71cc74f7090818f4/gnumpy-0.2.tar.gz\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->nolearn) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->nolearn) (1.16.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Theano->nolearn) (1.12.0)\n",
            "Building wheels for collected packages: nolearn, gdbn, gnumpy\n",
            "  Building wheel for nolearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/9c/b3/e8/a863ca29687669deabe68012f333d5c5bd6c9b5a3bea4f2538\n",
            "  Building wheel for gdbn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/aa/d7/7968a6696737f947ab854dc3394186a8fcfbd8aedcc8c66d07\n",
            "  Building wheel for gnumpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/61/c5/d7/74a2e7afea611ed0469c95bfb9d21f89dd73f1f4f9ecd7a784\n",
            "Successfully built nolearn gdbn gnumpy\n",
            "Installing collected packages: gnumpy, gdbn, nolearn\n",
            "Successfully installed gdbn-0.1 gnumpy-0.2 nolearn-0.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdNKmJUOXzoX",
        "colab_type": "code",
        "outputId": "51a7c564-3712-41f5-8e0d-1fd60071c6b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "source": [
        "from lasagne import layers\n",
        "from lasagne.updates import nesterov_momentum\n",
        "from nolearn.lasagne import NeuralNet\n",
        "\n",
        "net1 = NeuralNet(\n",
        "    layers=[  # three layers: one hidden layer\n",
        "        ('input', layers.InputLayer),\n",
        "        ('hidden', layers.DenseLayer),\n",
        "        ('output', layers.DenseLayer),\n",
        "        ],\n",
        "    # layer parameters:\n",
        "    input_shape=(None, 9216),  # 96x96 input pixels per batch, None translates to a variable batch size\n",
        "    hidden_num_units=100,  # number of units in hidden layer\n",
        "    output_nonlinearity=None,  # activation function is Rectifier\n",
        "    output_num_units=30,  # 30 target values\n",
        "\n",
        "    # optimization method:\n",
        "    update=nesterov_momentum,\n",
        "    update_learning_rate=0.01,\n",
        "    update_momentum=0.9,\n",
        "\n",
        "    regression=True,  # flag to indicate we're dealing with regression problem\n",
        "    max_epochs=400,  # we want to train this many epochs\n",
        "    verbose=1, # print out information during training\n",
        "    )\n",
        "\n",
        "X, Y = Data_PreProcess()\n",
        "net1.fit(X, Y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SkipTest",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSkipTest\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-2e20b4a680f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlasagne\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlasagne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnesterov_momentum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnolearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlasagne\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNeuralNet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m net1 = NeuralNet(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nolearn/lasagne/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m from .handlers import (\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mPrintLayerInfo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mPrintLog\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mRememberBestWeights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mSaveWeights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nolearn/lasagne/handlers.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mansi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_conv_infos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_conv2d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nolearn/lasagne/util.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmaxpoollayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mMaxPool2DLayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mlasagne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda_convnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConv2DCCLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mlasagne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda_convnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMaxPool2DCCLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mconvlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv2DCCLayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/lasagne/layers/cuda_convnet.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m                       \"up to Theano 0.9).\")  # pragma: no cover\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msandbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgpu_contiguous\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpylearn2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msandbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda_convnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_acts\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFilterActs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/theano/sandbox/cuda/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# the folder will be skipped by nosetests without failing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m raise SkipTest(\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;34m\"You are importing theano.sandbox.cuda. This is the old GPU back-end and \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;34m\"is removed from Theano. Use Theano 0.9 to use it. Even better, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;34m\"transition to the new GPU back-end! See \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSkipTest\u001b[0m: You are importing theano.sandbox.cuda. This is the old GPU back-end and is removed from Theano. Use Theano 0.9 to use it. Even better, transition to the new GPU back-end! See https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnu39HBLxS5a",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krq5Jk-lCfND",
        "colab_type": "text"
      },
      "source": [
        "### 6. kNN Classification & Scoring\n",
        "\n",
        "Make sure to add a commentary at the end with a summary of this section including what was done and what the result was (on Dev and Test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcT1qVjhCipV",
        "colab_type": "text"
      },
      "source": [
        "### 7. Multinomial Naive Bayes Classification & Scoring\n",
        "Make sure to add a commentary at the end with a summary of this section including what was done and what the result was (on Dev and Test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMU0QdgtCs-C",
        "colab_type": "text"
      },
      "source": [
        "### 8. Logistic Regression Classification & Scoring\n",
        "Make sure to add a commentary at the end with a summary of this section including what was done and what the result was (on Dev and Test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyAWw-EBC1Q1",
        "colab_type": "text"
      },
      "source": [
        "### 9. Neural Net Classification & Scoring  \n",
        "\n",
        "Note - must binarize the labels before processing.\n",
        "Make sure to add a commentary at the end with a summary of this section including what was done and what the result was (on Dev and Test)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vakj9sfvC6tM",
        "colab_type": "code",
        "outputId": "9334a380-296d-4e1e-de21-70de25182eb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "# Sample Code from Week 7 Tutorial - DON'T RUN AS IS, DATA SETS WON'T WORK\n",
        "# Single layer NN\n",
        "\n",
        "# Sample Binarization code:\n",
        "def binarizeY(data):\n",
        "    binarized_data = np.zeros((data.size,10))\n",
        "    for j in range(0,data.size):\n",
        "        feature = data[j:j+1]\n",
        "        i = feature.astype(np.int64) \n",
        "        binarized_data[j,i]=1\n",
        "    return binarized_data\n",
        "train_labels_b = binarizeY(train_labels)\n",
        "test_labels_b = binarizeY(test_labels)\n",
        "numClasses = train_labels_b[1].size\n",
        "print('Classes = %d' %(numClasses))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-612777d2367e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mbinarized_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbinarized_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrain_labels_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinarizeY\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mtest_labels_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinarizeY\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mnumClasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_labels_b\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_labels' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KV8bsseFWD_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## (1) Parameters \n",
        "# Initialize the weights to small, but non-zero, value\n",
        "z = (np.random.randn(*(numFeatures, numClasses))*0.01)\n",
        "print(z.shape, type(z))\n",
        "z2 = np.asarray(z)\n",
        "print(z2.shape, type(z2))\n",
        "w = theano.shared((np.random.randn(*(numFeatures, numClasses))*.01))\n",
        "print(w.ndim,type(w),type(w.get_value()))\n",
        "print(w.dtype)\n",
        "print(w.nonzero(), w.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "894pMKdeHR-b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## (2) Model\n",
        "# Theano objects accessed with standard Python variables\n",
        "# using Softmax - i.e. basically just a multi-class version of sigmoid activation\n",
        "X = T.matrix()\n",
        "Y = T.matrix()\n",
        "\n",
        "def model(X, w):\n",
        "    return T.nnet.softmax(T.dot(X, w))\n",
        "y_hat = model(X, w)\n",
        "print(y_hat,y_hat.dtype, y_hat.ndim, type(y_hat))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdwQ9H4DHdp-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## (3) Cost function\n",
        "cost = T.mean(T.nnet.categorical_crossentropy(y_hat, Y))\n",
        "print(cost, cost.dtype, cost.ndim, type(cost))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Qgi3AjrHjDA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## (4) Objective (and solver)\n",
        "\n",
        "alpha = 0.01\n",
        "gradient = T.grad(cost=cost, wrt=w) \n",
        "update = [[w, w - gradient * alpha]] \n",
        "train = theano.function(inputs=[X, Y], outputs=cost, updates=update, allow_input_downcast=True) # computes cost, then runs update\n",
        "y_pred = T.argmax(y_hat, axis=1) # select largest probability as prediction\n",
        "predict = theano.function(inputs=[X], outputs=y_pred, allow_input_downcast=True)\n",
        "\n",
        "def gradientDescent(epochs):\n",
        "    trainTime = 0.0\n",
        "    predictTime = 0.0\n",
        "    for i in range(epochs):\n",
        "        start_time = time.time()\n",
        "        cost = train(train_data[0:len(train_data)], train_labels_b[0:len(train_data)])\n",
        "        trainTime =  trainTime + (time.time() - start_time)\n",
        "        print('%d) accuracy = %.4f' %(i+1, np.mean(np.argmax(test_labels_b, axis=1) == predict(test_data))))\n",
        "    print('train time = %.2f' %(trainTime))\n",
        "\n",
        "gradientDescent(50)\n",
        "\n",
        "start_time = time.time()\n",
        "predict(test_data)   \n",
        "print('predict time = %.2f' %(time.time() - start_time))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ij12Ju3NHs6K",
        "colab_type": "text"
      },
      "source": [
        "Next Steps - do some/all/none of the following:\n",
        "- Try Stochastic and Mini-Batch \n",
        "- Try 2 and 3 layer models\n",
        "- Try using a different activation - like rectifier and/or max pooling\n",
        "- Try adding noise (like Dropouts) - can increase generalization and training speeds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCXn5fPUIK7j",
        "colab_type": "text"
      },
      "source": [
        "**FINALLY**: *Convolutional NN*\n",
        "\n",
        "- This is likely the prize - developed for image processing\n",
        "- Need to reprocess data - uses 2D images (not 1D)\n",
        "- Go to last section of Week7 Tutorial for code on Convolutional NNs\n",
        "- Possibly do a generative model to create images and further train the network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adQrLjkYIz7O",
        "colab_type": "text"
      },
      "source": [
        "### 10. Conclusions  \n",
        "\n",
        "Make sure to write up summary of different methods, results based on Dev and then on Test (maybe present in table). "
      ]
    }
  ]
}